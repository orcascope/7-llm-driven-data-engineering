{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNk9W630WupPQVxZLCTDQBZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orcascope/7-llm-driven-data-engineering/blob/main/setup_spark_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkYT6K7Bp2T5",
        "outputId": "98a7056e-ace5-4c2b-8230-db045c1e1683"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Github/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrIjg86XqYUs",
        "outputId": "f413658a-b865-402e-baa3-a9194c18a191"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Github\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUhBhrGmyAvs"
      },
      "source": [
        "!apt-get -qq update > /tmp/apt.out\n",
        "!apt-get install -y -qq openjdk-11-jdk-headless"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!(wget -q --show-progress -nc https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz)\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32LLPBQpij-w",
        "outputId": "e2e3bd5c-77e0-4fc9-d548-da4148d291cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark-3.2.1-bin-had 100%[===================>] 287.03M  21.5MB/s    in 14s     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF-e1DAsGUaL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c79bf616-c855-4cbb-d552-ae63fe1cbe45"
      },
      "source": [
        "try:\n",
        "  import pyspark, findspark, delta\n",
        "except:\n",
        "  %pip install -q --upgrade pyspark==3.2.1\n",
        "  %pip install -q findspark\n",
        "  %pip install -q delta"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.0/199.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for delta (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import pyspark\n",
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/drive/MyDrive/Github/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "findspark.init()\n",
        "MAX_MEMORY=\"8g\"\n",
        "maven_coords = [\n",
        "    \"org.apache.spark:spark-avro_2.12:3.2.1\",\n",
        "    \"io.delta:delta-core_2.12:2.0.0rc1\",\n",
        "    \"org.xerial:sqlite-jdbc:3.36.0.3\",\n",
        "    \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\",\n",
        "    \"com.acervera.osm4scala:osm4scala-spark3-shaded_2.12:1.0.8\",\n",
        "]\n",
        "spark = (pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
        "    .config(\"spark.jars.packages\", \",\".join(maven_coords))\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "    .config(\"spark.executor.memory\", MAX_MEMORY)\n",
        "    .config(\"spark.driver.memory\", MAX_MEMORY)\n",
        "    .enableHiveSupport()\n",
        "    .getOrCreate()\n",
        "    )\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "nyXkqLzrjJ1c",
        "outputId": "a68acf0b-b7ab-458f-9cc2-231c6ee211ab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7940746d0550>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4a9920ae40ad:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>MyApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from delta.tables import DeltaTable\n",
        "import delta\n",
        "\n",
        "df = spark.createDataFrame([{'s':'hello world','i':1234}])\n",
        "\n",
        "(df.write.format('delta')\n",
        "         .mode('overwrite')\n",
        "         .option(\"mergeSchema\", \"true\")\n",
        "         .save('./delta_hello_world')\n",
        ")\n"
      ],
      "metadata": {
        "id": "magAGym1jM40"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.format(\"delta\").load('./delta_hello_world').createOrReplaceTempView(\"delta_hello_world\")\n",
        "df2 = spark.sql(\"\"\"\n",
        "  select * from delta_hello_world\n",
        "\"\"\")\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-Guj5fywDJt",
        "outputId": "173891ad-5521-4b2a-d700-a1a27492fb09"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----------+\n",
            "|   i|          s|\n",
            "+----+-----------+\n",
            "|1234|hello world|\n",
            "+----+-----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}